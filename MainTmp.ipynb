{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetRoot = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log2(x):\n",
    "    return int(math.log2(x))\n",
    "\n",
    "def plot_images(images, log2_res, fname=\"\"):\n",
    "    '''\n",
    "    Helper function to plot a set of images\n",
    "    '''\n",
    "    scales = {2: 0.5, 3: 1, 4: 2, 5: 3, 6: 4, 7: 5, 8: 6, 9: 7, 10: 8}\n",
    "    scale = scales[log2_res]\n",
    "\n",
    "    grid_col = min(images.shape[0], int(32 // scale))\n",
    "    grid_row = 1\n",
    "\n",
    "    f, axarr = plt.subplots(\n",
    "        grid_row, grid_col, figsize=(grid_col * scale, grid_row * scale)\n",
    "    )\n",
    "\n",
    "    for row in range(grid_row):\n",
    "        ax = axarr if grid_row == 1 else axarr[row]\n",
    "        for col in range(grid_col):\n",
    "            ax[col].imshow(images[row * grid_col + col])\n",
    "            ax[col].axis(\"off\")\n",
    "    plt.show()\n",
    "    if fname:\n",
    "        f.savefig(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFHQDataset(Dataset):\n",
    "    \"\"\" Dataset to load FFHQ data from a dataframe\n",
    "\n",
    "    Args:\n",
    "\n",
    "        dataframe : dataframe with image paths in column 'image_path'\n",
    "        datasetRoot : the root path to join before the image_path, if any.\n",
    "        res : resolution of the image generated\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, datasetRoot, res = 2, device = \"cuda:0\") -> None:\n",
    "        super().__init__()\n",
    "        self.dataframe = dataframe\n",
    "        self.datasetRoot = datasetRoot\n",
    "        self.res = res\n",
    "        self.device = torch.device(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "        img_path = os.path.join(self.datasetRoot,self.dataframe['image_path'].iloc[index])\n",
    "        image = read_image(img_path)\n",
    "        return Resize((self.res,self.res))(image).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fade_in(alpha, a, b):\n",
    "    '''\n",
    "    Smoothing function\n",
    "    '''\n",
    "    return alpha * a + (1- alpha ) * b\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    '''\n",
    "    Wasserstein Loss ( Refer to WGANs)\n",
    "    '''\n",
    "    return - (y_true * y_pred).mean()\n",
    "\n",
    "def pixel_norm(x, epsilon = 1e-8):\n",
    "    return x / torch.sqrt(torch.mean(x**2, dim = -1, keepdim=True)+ epsilon)\n",
    "\n",
    "def minibatch_std(x, group_size=4, num_new_features=1, eps=1e-8):\n",
    "    \"\"\"Compute the mini-batch standard deviation of input tensor x.\n",
    "        ref : <https://arxiv.org/pdf/1710.10196.pdf>\n",
    "    \"\"\"\n",
    "    batch_size, num_channels, height, width = x.shape\n",
    "    grouped_size = min(group_size, batch_size)\n",
    "    grouped_inputs = x.view(grouped_size, -1, num_channels, height, width)\n",
    "    \n",
    "    grouped_mean = grouped_inputs.mean(dim=0, keepdim=False)\n",
    "    squared_diffs = (grouped_inputs - grouped_mean)**2\n",
    "    grouped_var = squared_diffs.mean(dim=0, keepdim=False)\n",
    "    grouped_std = torch.sqrt(grouped_var + eps)\n",
    "    \n",
    "    avg_std = grouped_std.mean(dim=[1,2,3], keepdim=True)\n",
    "    tiled_avg_std = avg_std.repeat(grouped_size, 1, height, width)\n",
    "    new_features = torch.cat((x, tiled_avg_std), dim=1)\n",
    "    \n",
    "    return new_features\n",
    "\n",
    "class EqualisedConv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel = 3, gain = 2) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.w = nn.Parameter(torch.empty(out_channels, in_channels ,kernel, kernel).normal_(), requires_grad= True)\n",
    "        self.b = nn.Parameter(torch.empty(out_channels).zero_() , requires_grad= True)\n",
    "\n",
    "        fan_in = kernel * kernel * in_channels\n",
    "        self.scale = math.sqrt(gain / fan_in)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        return F.conv2d(inputs, self.scale * self.w, self.b, padding='same')\n",
    "\n",
    "\n",
    "class EqualisedLinear(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out , gain = 2, lr_multiplier = 1) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.w = nn.Parameter(torch.empty(dim_out,dim_in).normal_(0.0, 1.0 / lr_multiplier), requires_grad= True)\n",
    "        self.b = nn.Parameter(torch.empty(dim_out).zero_(),requires_grad=True)\n",
    "\n",
    "        self.scale = math.sqrt(gain / dim_in)\n",
    "        self.lr_multiplier = lr_multiplier\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        return self.lr_multiplier * F.linear(inputs, self.scale * self.w, self.b) \n",
    "\n",
    "class AddNoise(nn.Module):\n",
    "    def __init__(self, x_channels) -> None:\n",
    "        super().__init__()\n",
    "        self.b = nn.Parameter(torch.empty([1,x_channels,1,1]).normal_(),requires_grad=True)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x, noise = inputs\n",
    "        return  x + self.b * noise\n",
    "\n",
    "class AdaIn(nn.Module):\n",
    "    def __init__(self, w_channels, x_channels, gain =1) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.x_channels = x_channels\n",
    "\n",
    "        self.dense_1 = EqualisedLinear(w_channels, x_channels, gain)\n",
    "        self.dense_2 = EqualisedLinear(w_channels, x_channels, gain)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x, w = inputs\n",
    "        ys = self.dense_1(w).reshape((-1,self.x_channels,1,1))\n",
    "        yb = self.dense_2(w).reshape((-1,self.x_channels,1,1))\n",
    "\n",
    "        return ys * x + yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorBlock(nn.Module):\n",
    "    def __init__(self, filter_num, res, input_shape, is_base) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_base = is_base\n",
    "\n",
    "        print(\"Base\" if is_base else \"\",filter_num, \" : \", input_shape)\n",
    "\n",
    "        # Needs AdaIn & AddNoise Layers\n",
    "        if not is_base:\n",
    "            self.up_sample = nn.Upsample(scale_factor=2)\n",
    "            self.init_conv = EqualisedConv2D(input_shape[0], filter_num, 3)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.add_noise_1 = AddNoise(input_shape[0])\n",
    "        self.instance_norm_1 = nn.InstanceNorm2d(input_shape[0])\n",
    "        self.ada_in_1 = AdaIn(512, input_shape[0])\n",
    "        self.conv_1 = EqualisedConv2D(input_shape[0], filter_num, 3)\n",
    "\n",
    "        self.add_noise_2 = AddNoise(input_shape[0])\n",
    "        self.instance_norm_2 = nn.InstanceNorm2d(input_shape[0])\n",
    "        self.ada_in_2 = AdaIn(512, input_shape[0])\n",
    "        self.conv_2 = EqualisedConv2D(input_shape[0], filter_num, 3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x, w, noise = inputs\n",
    "\n",
    "        if not self.is_base:\n",
    "            x = self.up_sample(x)\n",
    "            x = self.init_conv(x)\n",
    "\n",
    "        x = self.add_noise_1([x, noise])\n",
    "        x = self.instance_norm_1(x)\n",
    "        x = self.ada_in_1([x,w])\n",
    "        x = self.conv_1(x)\n",
    "\n",
    "        x = self.add_noise_2([x, noise])\n",
    "        x = self.instance_norm_2(x)\n",
    "        x = self.ada_in_2([x,w])\n",
    "        x = self.conv_2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# #### TEST Generate BLOCK ####\n",
    "# # x = GeneratorBlock(512,4,[512,2,2],True)([torch.ones(24,512,2,2),torch.ones(24,512),torch.ones(1,2,2)])\n",
    "# # x = EqualisedConv2D(512,3,1,gain = 1)(x)\n",
    "# # x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorBlock(nn.Module):\n",
    "    def __init__(self, filter_num_1, filter_num_2, res, is_base = False) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_base = is_base\n",
    "        self.leakyRelu = nn.LeakyReLU(0.2)\n",
    "\n",
    "        if not is_base:\n",
    "            self.conv_1 = EqualisedConv2D(filter_num_1,filter_num_1,3)\n",
    "            self.conv_2 = EqualisedConv2D(filter_num_1,filter_num_2)\n",
    "            self.avg_pool = nn.AvgPool2d((2,2))\n",
    "        else:\n",
    "            self.conv_1 = EqualisedConv2D(filter_num_1+1,filter_num_1,3)\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.lin_1 = EqualisedLinear(res*res*filter_num_1,filter_num_1)\n",
    "            self.lin_2 = EqualisedLinear(filter_num_1,1)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # if not base, we don't flatten\n",
    "        if not self.is_base:\n",
    "            x = self.conv_1(inputs)\n",
    "            x = self.leakyRelu(x)\n",
    "            x = self.conv_2(x)\n",
    "            x = self.leakyRelu(x)\n",
    "            x = self.avg_pool(x)\n",
    "            return x\n",
    "        else:\n",
    "        # if base, we flatten & pass through dense\n",
    "            x = minibatch_std(inputs)\n",
    "            x = self.conv_1(x)\n",
    "            x = self.leakyRelu(x)\n",
    "            x = self.flatten(x)\n",
    "            x = self.lin_1(x)\n",
    "            x = self.leakyRelu(x)\n",
    "            x = self.lin_2(x)\n",
    "            return x\n",
    "        \n",
    "#### TEST Dicriminator BLOCK ####\n",
    "# DiscriminatorBlock(5,5,4,True)(torch.ones(24,5,4,4)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator Module for the StyleGAN Generator\n",
    "    \"\"\"\n",
    "    def __init__(self, start_res_log2, target_res_log2) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.start_res_log2 = start_res_log2\n",
    "        self.target_res_log2 = target_res_log2\n",
    "        self.num_stages = target_res_log2 - start_res_log2 +1\n",
    "\n",
    "        self.curr_res_log2 = start_res_log2\n",
    "\n",
    "        # generator Blocks\n",
    "        self.g_blocks = []\n",
    "        # one rgb block per genblock\n",
    "        self.to_rgb = []\n",
    "\n",
    "        self.filter_nums = {\n",
    "            0: 512,\n",
    "            1: 512,\n",
    "            2: 512,  # 4x4\n",
    "            3: 512,  # 8x8\n",
    "            4: 512,  # 16x16\n",
    "            5: 512,  # 32x32\n",
    "            6: 256,  # 64x64\n",
    "            7: 128,  # 128x128\n",
    "            8: 64,  # 256x256\n",
    "            9: 32,  # 512x512\n",
    "            10: 16,\n",
    "        }\n",
    "\n",
    "        start_res = 2 ** start_res_log2\n",
    "        self.input_shape = (self.filter_nums[start_res_log2], start_res, start_res)\n",
    "        \n",
    "        for i in range(start_res_log2, target_res_log2 + 1):\n",
    "            filter_num = self.filter_nums[i]\n",
    "            res = 2 ** i\n",
    "            \n",
    "            to_rgb = EqualisedConv2D(filter_num , 3, 1, gain=1)\n",
    "\n",
    "            self.to_rgb.append(to_rgb)\n",
    "\n",
    "            is_base = i == self.start_res_log2\n",
    "\n",
    "            if is_base:\n",
    "                input_shape = (self.filter_nums[i], res, res)\n",
    "            else:\n",
    "                input_shape = (self.filter_nums[i - 1], 2 ** (i - 1), 2 ** (i - 1))\n",
    "            \n",
    "            g_block = GeneratorBlock(filter_num, res, input_shape, is_base)\n",
    "            self.g_blocks.append(g_block)\n",
    "\n",
    "        self.up_sample = nn.Upsample(scale_factor=2)\n",
    "        self.g_blocks = nn.ModuleList(self.g_blocks)\n",
    "        self.to_rgb = nn.ModuleList(self.to_rgb)\n",
    "    \n",
    "    def grow(self, res_log2):\n",
    "        self.curr_res_log2 = res_log2\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        g_input, w, noise_inputs, alpha = inputs\n",
    "\n",
    "        \"\"\"\n",
    "        Shapes: \n",
    "            g_input -> self.filter_nums[start_res_log2], start_res, start_res\n",
    "            w -> self.num_stages, 512\n",
    "            noise_inputs -> self.num_stages, 1, res, res\n",
    "            alpha -> 1\n",
    "        \"\"\"\n",
    "        num_stages = self.curr_res_log2 - self.start_res_log2 +1\n",
    "\n",
    "        x = self.g_blocks[0]([g_input, w[:,0],noise_inputs[0]])\n",
    "\n",
    "        rgb = None\n",
    "\n",
    "        if num_stages==1:\n",
    "            rgb = self.to_rgb[0](x)\n",
    "        else:\n",
    "            for i in range(1,num_stages-1):\n",
    "                x = self.g_blocks[i]([x,w[:,i],noise_inputs[i]])\n",
    "            \n",
    "            old_rgb = self.to_rgb[num_stages-2](x)\n",
    "            old_rgb = self.up_sample(old_rgb)\n",
    "\n",
    "            i = num_stages -1\n",
    "            x = self.g_blocks[i]([x,w[:,i],noise_inputs[i]])\n",
    "\n",
    "            new_rgb = self.to_rgb[i](x)\n",
    "\n",
    "            rgb = fade_in(alpha[0],new_rgb, old_rgb)\n",
    "        \n",
    "        return rgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, start_res_log2, target_res_log2) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.start_res_log2 = start_res_log2\n",
    "        self.target_res_log2 = target_res_log2\n",
    "        self.curr_res_log2 = self.start_res_log2\n",
    "\n",
    "        self.num_stages = target_res_log2 - start_res_log2 +1\n",
    "\n",
    "        self.filter_nums = {\n",
    "            0: 512,\n",
    "            1: 512,\n",
    "            2: 512,  # 4x4\n",
    "            3: 512,  # 8x8\n",
    "            4: 512,  # 16x16\n",
    "            5: 512,  # 32x32\n",
    "            6: 256,  # 64x64\n",
    "            7: 128,  # 128x128\n",
    "            8: 64,  # 256x256\n",
    "            9: 32,  # 512x512train_\n",
    "            10: 16,\n",
    "        }\n",
    "\n",
    "        self.d_blocks = []\n",
    "\n",
    "        self.from_rgb = []\n",
    "\n",
    "        for res_log2 in range(self.start_res_log2, self.target_res_log2 +1):\n",
    "            res = 2**res_log2\n",
    "\n",
    "            filter_num = self.filter_nums[res_log2]\n",
    "\n",
    "            from_rgb = nn.Sequential(\n",
    "                EqualisedConv2D(3,filter_num,1),\n",
    "                nn.LeakyReLU(0.2)\n",
    "            )\n",
    "\n",
    "            self.from_rgb.append(from_rgb)\n",
    "\n",
    "            d_block = DiscriminatorBlock(filter_num, self.filter_nums[res_log2-1],res,len(self.d_blocks)==0)\n",
    "            self.d_blocks.append(d_block)\n",
    "            \n",
    "        self.avg_pool = nn.AvgPool2d((2,2))\n",
    "        self.from_rgb = nn.ModuleList(self.from_rgb)\n",
    "        self.d_blocks = nn.ModuleList(self.d_blocks)\n",
    "    \n",
    "    def grow(self, res_log2):\n",
    "        self.curr_res_log2 = res_log2\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        input_image, alpha = inputs\n",
    "\n",
    "        res = 2 ** self.curr_res_log2 \n",
    "        idx = self.curr_res_log2 - self.start_res_log2\n",
    "\n",
    "        x = self.from_rgb[idx](input_image)\n",
    "        x = self.d_blocks[idx](x)\n",
    "\n",
    "        if idx >0 :\n",
    "            idx -=1\n",
    "            downsized_image = self.avg_pool(input_image)\n",
    "            y = self.from_rgb[idx](downsized_image)\n",
    "            x = fade_in(alpha[0],x,y)\n",
    "\n",
    "            for i in range(idx, -1, -1):\n",
    "                x = self.d_blocks[i](x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mapping(nn.Module):\n",
    "    def __init__(self, num_stages, input_shape = 512) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_stages = num_stages\n",
    "\n",
    "        self.layers = []\n",
    "        for _ in range(8):\n",
    "            self.layers.append(EqualisedLinear(input_shape, 512, 0.01))\n",
    "            self.layers.append(nn.LeakyReLU(0.2))\n",
    "        \n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = pixel_norm(input)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return torch.tile(torch.unsqueeze(x,1),(1,self.num_stages,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phase(Enum):\n",
    "    STABLE = 2\n",
    "    TRANSITION = 1\n",
    "\n",
    "class OptimType(Enum):\n",
    "    ADAM = 1\n",
    "    \n",
    "class StyleGAN(nn.Module):\n",
    "    def __init__(self, z_dim = 512, target_res = 128, start_res = 4) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.z_dim = 512\n",
    "\n",
    "        self.target_res_log2 = log2(target_res)\n",
    "        self.start_res_log2 = log2(start_res)\n",
    "\n",
    "        self.curr_res_log2 = self.target_res_log2\n",
    "        self.num_stages = self.target_res_log2 - self.start_res_log2 +1\n",
    "\n",
    "        self.alpha = 1.0\n",
    "        self.train_step_counter = 0\n",
    "        \n",
    "\n",
    "        self.mapping = Mapping(self.num_stages, self.z_dim)\n",
    "        self.discriminator = Discriminator(self.start_res_log2, self.target_res_log2)\n",
    "        self.generator = Generator(self.start_res_log2,self.target_res_log2)\n",
    "\n",
    "        self.g_input_shape = self.generator.input_shape\n",
    "\n",
    "        self.phase = None\n",
    "\n",
    "        self.loss_weights = {\n",
    "            \"gradient_penalty\" : 10,\n",
    "            \"drift\" : 0.001\n",
    "        }\n",
    "    \n",
    "        \n",
    "    def grow_model(self, res):\n",
    "        res_log2 = log2(res)\n",
    "\n",
    "        self.generator.grow(res_log2)\n",
    "        self.discriminator.grow(res_log2)\n",
    "\n",
    "        self.curr_res_log2 = res_log2\n",
    "        print(f\"\\nModel resolution : {res}x{res}\")\n",
    "\n",
    "    def configure(self, steps_per_epoch, phase, res, d_optimizer : OptimType, g_optimizer: OptimType, *args, **kwargs):\n",
    "        self.loss_weights = kwargs.pop(\"loss_weights\",self.loss_weights)\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        if res!= 2 ** self.curr_res_log2:\n",
    "            self.grow_model(res)\n",
    "            if d_optimizer == OptimType.ADAM:\n",
    "                self.d_optimizer = torch.optim.Adam(params = self.discriminator.parameters(), lr=1e-3, betas=(0.0,0.99),eps=1e-8)\n",
    "            if g_optimizer == OptimType.ADAM:\n",
    "                self.g_optimizer = torch.optim.Adam(params = self.generator.parameters(), lr=1e-3, betas=(0.0,0.99),eps=1e-8)\n",
    "        \n",
    "        self.train_step_counter = 0\n",
    "        self.phase = phase\n",
    "        self.d_loss_metric = torch.mean\n",
    "        self.g_loss_metric = torch.mean\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "    \n",
    "    def generate_noise(self,batch_size, device):\n",
    "        return [\n",
    "            torch.empty(batch_size,1, 2**res,2**res ).normal_().to(device)\n",
    "            for res in range(self.start_res_log2, self.target_res_log2+1)\n",
    "        ]\n",
    "    \n",
    "    def gradient_penalty(self, real_images : torch.Tensor, fake_images: torch.Tensor, alpha, batch_size):\n",
    "        epsilon = torch.empty(batch_size,1,1,1).to(real_images.get_device())\n",
    "\n",
    "        interpolated_images = epsilon * real_images + (1 - epsilon ) * fake_images\n",
    "        interpolated_images.requires_grad_(True)\n",
    "\n",
    "        mixed_scores = self.discriminator([interpolated_images,alpha])\n",
    "\n",
    "        loss_inter = wasserstein_loss(-torch.ones(batch_size).to(real_images.get_device()), mixed_scores)\n",
    "\n",
    "        gradient = torch.autograd.grad(\n",
    "            inputs = interpolated_images,\n",
    "            outputs = loss_inter,\n",
    "            grad_outputs = torch.ones_like(loss_inter),\n",
    "            create_graph = True,\n",
    "            retain_graph = True \n",
    "        )[0]\n",
    "\n",
    "        gradient = gradient.view(gradient.shape[0],-1)\n",
    "        gradient_norm = gradient.norm(2, dim=1)\n",
    "        gradient_penalty = torch.mean((gradient_norm - 1)**2)\n",
    "        return gradient_penalty\n",
    "\n",
    "    def forward(self, inputs : dict(), device : torch.device):\n",
    "        style_code = inputs.get(\"style_code\", None)\n",
    "        z = inputs.get(\"z\", None)\n",
    "        noise = inputs.get(\"noise\", None)\n",
    "        batch_size = inputs.get(\"batch_size\", 1)\n",
    "        alpha = inputs.get(\"alpha\", 1.0)\n",
    "        alpha = torch.Tensor([alpha]).to(device)\n",
    "        if style_code is None:\n",
    "            if z is None:\n",
    "                z = torch.empty(batch_size, self.z_dim).normal_().to(device)\n",
    "            style_code = self.mapping(z)\n",
    "        if noise is None:\n",
    "            noise = self.generate_noise(batch_size, device)\n",
    "\n",
    "        const_input = torch.ones(tuple([batch_size]+ list(self.g_input_shape))).to(device)\n",
    "        images = self.generator([const_input, style_code, noise, alpha]).cpu().detach().numpy()\n",
    "        images = np.clip((images * 0.5 + 0.5) * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "        return images\n",
    "    \n",
    "\n",
    "    def train_step(self, real_images : torch.Tensor):\n",
    "        self.g_optimizer.zero_grad()\n",
    "        self.train_step_counter +=1\n",
    "        \n",
    "        if self.phase == Phase.TRANSITION:\n",
    "            self.alpha = float(self.train_step_counter/ self.steps_per_epoch)\n",
    "        elif self.phase == Phase.STABLE:\n",
    "            self.alpha = 1.0\n",
    "        else:\n",
    "            raise NotImplementedError(\"Configure Model before training\")\n",
    "\n",
    "        alpha = torch.Tensor([self.alpha])\n",
    "        batch_size = real_images.shape[0]\n",
    "\n",
    "        real_labels = torch.ones(batch_size).to(real_images.get_device())\n",
    "        fake_labels = -torch.ones(batch_size).to(real_images.get_device())\n",
    "\n",
    "        z = torch.empty(batch_size, self.z_dim).normal_().to(real_images.get_device())\n",
    "        const_input = torch.ones(tuple([batch_size]+ list(self.g_input_shape))).to(real_images.get_device())\n",
    "        noise = self.generate_noise(batch_size,real_images.get_device())\n",
    "\n",
    "        w = self.mapping(z)\n",
    "        fake_images : torch.Tensor = self.generator([const_input, w, noise, alpha])\n",
    "\n",
    "        pred_fake = self.discriminator([fake_images, alpha])\n",
    "        \n",
    "        #### Generator Loss\n",
    "        g_loss = wasserstein_loss(real_labels, pred_fake)\n",
    "\n",
    "        self.generator.zero_grad()\n",
    "        g_loss.backward()\n",
    "        self.g_optimizer.step()\n",
    "\n",
    "        ### Discriminator Loss\n",
    "        pred_fake = self.discriminator([fake_images.detach(), alpha])\n",
    "        pred_real = self.discriminator([real_images.float(), alpha])\n",
    "\n",
    "        loss_fake = wasserstein_loss(fake_labels, pred_fake)\n",
    "        loss_real = wasserstein_loss(real_labels, pred_real)\n",
    "\n",
    "        gradient_penalty = self.gradient_penalty(real_images, fake_images.detach(), alpha, batch_size)\n",
    "\n",
    "        all_pred = torch.concat([pred_fake, pred_real], dim=0)\n",
    "        drift_loss = self.loss_weights[\"drift\"] * torch.mean(all_pred ** 2)\n",
    "        \n",
    "        d_loss = loss_fake + loss_real + gradient_penalty + drift_loss\n",
    "\n",
    "        self.discriminator.zero_grad()\n",
    "        d_loss.backward()\n",
    "        self.d_optimizer.step()\n",
    "\n",
    "        return {\n",
    "            \"g_loss\": torch.mean(g_loss),\n",
    "            \"d_loss\": torch.mean(d_loss)\n",
    "        }\n",
    "    \n",
    "    def save(self,batch_num,file_path):\n",
    "        states = {\n",
    "            \"res\": self.curr_res_log2,\n",
    "            \"batch_num\" : batch_num,\n",
    "            \"phase\" : self.phase,\n",
    "            \"weights\" : self.state_dict()\n",
    "        }\n",
    "        \n",
    "        pickle.dump(states,open(file_path,\"wb\"))\n",
    "    \n",
    "    def load(self,file_path):\n",
    "        states = pickle.load(open(file_path,\"rb\"))\n",
    "        \n",
    "        self.load_state_dict(states[\"weights\"])\n",
    "        \n",
    "        return states[\"res\"],states[\"batch_num\"],states[\"phase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base 512  :  (512, 4, 4)\n",
      " 512  :  (512, 4, 4)\n",
      " 512  :  (512, 8, 8)\n",
      " 512  :  (512, 16, 16)\n",
      " 256  :  (512, 32, 32)\n",
      " 128  :  (256, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "START_RES = 4\n",
    "TARGET_RES = 128\n",
    "\n",
    "# del style_gan\n",
    "\n",
    "style_gan = StyleGAN(start_res=START_RES, target_res= TARGET_RES).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use different batch size for different resolution, so larger image size\n",
    "# could fit into GPU memory. The keys is image resolution in log2\n",
    "batch_sizes = {2: 16, 3: 16, 4: 16, 5: 16, 6: 16, 7: 8, 8: 4, 9: 2, 10: 1}\n",
    "# We adjust the train step accordingly\n",
    "train_step_ratio = {k: batch_sizes[2] / v for k, v in batch_sizes.items()}\n",
    "\n",
    "def train(start_res = START_RES, target_res = TARGET_RES, steps_per_epoch = 5000, display_images = True, datasetRoot = \"./\", save_prefix = \"./style_gan\"):\n",
    "    val_batch_size = 16\n",
    "    val_z = torch.empty((val_batch_size, style_gan.z_dim)).normal_()\n",
    "    val_noise = style_gan.generate_noise(val_batch_size, torch.device(\"cuda:0\"))\n",
    "\n",
    "    def process_filepath(x):\n",
    "        x = x.split(\"/\")\n",
    "        x.pop(1)\n",
    "        return \"/\".join(x)\n",
    "\n",
    "    data = pd.read_json(os.path.join(datasetRoot,\"ffhq-dataset-v2.json\"),orient=\"index\")\n",
    "    data['image_path'] = data.apply(lambda x : process_filepath(x['thumbnail']['file_path']),axis=1)\n",
    "    data = data.drop(columns=['image','thumbnail','in_the_wild','metadata'])\n",
    "\n",
    "    train_data = data.loc[data['category']=='training'].drop(columns=['category'])\n",
    "    validation_data = data.loc[data['category']=='validation'].drop(columns=['category']).reset_index().drop(columns=['index'])\n",
    "\n",
    "    \n",
    "    start_res_log2 = log2(start_res)\n",
    "    target_res_log2 = log2(target_res)\n",
    "    \n",
    "    model_res, batch_num, model_phase = None,None,None\n",
    "    if os.path.exists(save_prefix):\n",
    "        print(\"Resuming Training\")\n",
    "        model_res, batch_num, model_phase = style_gan.load(save_prefix)\n",
    "\n",
    "    for res_log2 in range(start_res_log2, target_res_log2 + 1):\n",
    "        res = 2**res_log2\n",
    "        \n",
    "        if model_res is not None and res_log2 < model_res:\n",
    "            continue\n",
    "\n",
    "        for phase in [Phase.TRANSITION, Phase.STABLE]:\n",
    "            if  model_phase is not None and phase!=model_phase:\n",
    "                continue\n",
    "            if res==start_res and phase == Phase.TRANSITION:\n",
    "                continue\n",
    "            \n",
    "            train_dataloader = DataLoader(FFHQDataset(train_data, datasetRoot, res),batch_size= batch_sizes[res_log2], shuffle= True)\n",
    "            steps = int(train_step_ratio[res_log2] * steps_per_epoch)\n",
    "\n",
    "            style_gan.configure(\n",
    "                d_optimizer=OptimType.ADAM,\n",
    "                g_optimizer=OptimType.ADAM,\n",
    "                steps_per_epoch= steps,\n",
    "                res = res,\n",
    "                phase=phase\n",
    "            )\n",
    "\n",
    "            prefix = f\"res_{res}x{res}_{style_gan.phase}\"\n",
    "\n",
    "            print('Training '+prefix +\" model\")\n",
    "\n",
    "            g_loss = 0.0\n",
    "            d_loss = 0.0\n",
    "\n",
    "            ## Train for an epoch\n",
    "            for i, batch in tqdm(enumerate(train_dataloader),total=steps):\n",
    "                if batch_num is not None and i<batch_num:\n",
    "                    continue\n",
    "                if i==steps:\n",
    "                    break\n",
    "\n",
    "                losses = style_gan.train_step(batch)\n",
    "\n",
    "                if (i+1)%100==0:\n",
    "                    style_gan.save(i,save_prefix)\n",
    "                    # images = style_gan({\"z\": val_z, \"noise\": val_noise, \"alpha\": 1.0}, torch.device(\"cuda:0\"))\n",
    "                    # plot_images(images, res_log2)\n",
    "                    \n",
    "\n",
    "                g_loss += losses['g_loss']\n",
    "                d_loss += losses['d_loss']\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming Training\n",
      "\n",
      "Model resolution : 32x32\n",
      "Training res_32x32_Phase.STABLE model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████▎         | 3750/5000 [18:03<06:01,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model resolution : 64x64\n",
      "Training res_64x64_Phase.STABLE model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████▉                            | 1399/5000 [00:40<01:43, 34.71it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 70\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(start_res, target_res, steps_per_epoch, display_images, datasetRoot, save_prefix)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m==\u001b[39msteps:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mstyle_gan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     73\u001b[0m     style_gan\u001b[38;5;241m.\u001b[39msave(i,save_prefix)\n",
      "Cell \u001b[0;32mIn[11], line 139\u001b[0m, in \u001b[0;36mStyleGAN.train_step\u001b[0;34m(self, real_images)\u001b[0m\n\u001b[1;32m    136\u001b[0m noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_noise(batch_size,real_images\u001b[38;5;241m.\u001b[39mget_device())\n\u001b[1;32m    138\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapping(z)\n\u001b[0;32m--> 139\u001b[0m fake_images : torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconst_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m pred_fake \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator([fake_images, alpha])\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m#### Generator Loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[8], line 87\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     84\u001b[0m old_rgb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_sample(old_rgb)\n\u001b[1;32m     86\u001b[0m i \u001b[38;5;241m=\u001b[39m num_stages \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 87\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mg_blocks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnoise_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m new_rgb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_rgb[i](x)\n\u001b[1;32m     91\u001b[0m rgb \u001b[38;5;241m=\u001b[39m fade_in(alpha[\u001b[38;5;241m0\u001b[39m],new_rgb, old_rgb)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 33\u001b[0m, in \u001b[0;36mGeneratorBlock.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_sample(x)\n\u001b[1;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_conv(x)\n\u001b[0;32m---> 33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_noise_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance_norm_1(x)\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mada_in_1([x,w])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[5], line 69\u001b[0m, in \u001b[0;36mAddNoise.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m     68\u001b[0m     x, noise \u001b[38;5;241m=\u001b[39m inputs\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
