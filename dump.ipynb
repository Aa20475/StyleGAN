{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fade_in(alpha, a, b):\n",
    "    '''\n",
    "    Smoothing function\n",
    "    '''\n",
    "    return alpha * a + (1- alpha ) * b\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    '''\n",
    "    Wasserstein Loss ( Refer to WGANs)\n",
    "    '''\n",
    "    return - (y_true * y_pred).mean()\n",
    "\n",
    "def pixel_norm(x, epsilon = 1e-8):\n",
    "    return x / torch.sqrt(torch.mean(x**2, dim = -1, keepdim=True)+ epsilon)\n",
    "\n",
    "def minibatch_std(tensor_input : torch.Tensor, epsilon = 1e-8):\n",
    "    '''\n",
    "    Minibatch Standard Deviation (ref : <https://arxiv.org/pdf/1710.10196.pdf>)\n",
    "    '''\n",
    "    n, c, h, w,  = tensor_input.shape\n",
    "\n",
    "    # shape into minibatches of size 4\n",
    "    group_size = min(4,n)\n",
    "    x = torch.reshape(tensor_input,[group_size,-1, c, h, w]).float()\n",
    "    num_batches = x.shape[1]\n",
    "\n",
    "    # calculate group standard deviation\n",
    "    group_var = x.var(0,unbiased=False)\n",
    "    group_std = torch.sqrt(group_var+epsilon)\n",
    "\n",
    "\n",
    "    # average deviation per minibatch\n",
    "    avg_std = torch.mean(group_std, dim=[1,2,3], keepdim=True)\n",
    "    avg_std = avg_std.repeat(group_size,num_batches,1,h,w)\n",
    "\n",
    "\n",
    "    # adding channel with mean of std of mini-batches\n",
    "    return torch.cat([x,avg_std],dim=2).reshape(n,c+1,h,w)\n",
    "\n",
    "\n",
    "\n",
    "class EqualizedConv2D(nn.Module):\n",
    "    def __init__(self, out_channels=1, kernel = 3, gain = 2, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.initialized = False\n",
    "        self.kernel = kernel\n",
    "        self.out_channels = out_channels\n",
    "        self.gain = gain\n",
    "\n",
    "        self.register_parameter('weights',None)\n",
    "        self.register_parameter('bias',None)\n",
    "\n",
    "    \n",
    "    def build(self, input : torch.Tensor):\n",
    "        self.in_channels = input.shape[1]\n",
    "\n",
    "        self.weights = nn.Parameter(input.new(self.out_channels, self.in_channels,self.kernel,self.kernel).normal_())\n",
    "        self.bias = nn.Parameter(input.new(self.out_channels).zero_())\n",
    "\n",
    "        fan_in = self.kernel * self.kernel * self.in_channels\n",
    "        self.scale = math.sqrt(self.gain / fan_in)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        if not self.initialized:\n",
    "            self.build(X)\n",
    "            self.initialized = True\n",
    "        return F.conv2d(X, self.weights, self.bias, padding='same')\n",
    "\n",
    "class EqualizedDense(nn.Module):\n",
    "    def __init__(self, dim_out, gain = 2, lr_multiplier = 1.0, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.initialized = False\n",
    "        self.dim_out = dim_out\n",
    "        self.gain = gain\n",
    "        self.lr_multiplier = lr_multiplier\n",
    "\n",
    "        self.register_parameter('weights',None)\n",
    "        self.register_parameter('bias',None)\n",
    "\n",
    "    def build(self, input : torch.Tensor):\n",
    "        self.dim_in = input.shape[-1]\n",
    "\n",
    "        self.weights = nn.Parameter(input.new(self.dim_out, self.dim_in).normal_(0.0,1.0/ self.lr_multiplier))\n",
    "        self.bias = nn.Parameter(input.new(self.dim_out).zero_())\n",
    "        \n",
    "        self.scale = math.sqrt(self.gain / self.dim_in)\n",
    "\n",
    "    def forward(self, X):\n",
    "        print(self.initialized)\n",
    "        if not self.initialized:\n",
    "            self.build(X)\n",
    "            self.initialized = True\n",
    "        \n",
    "        return F.linear(X, self.scale * self.weights, self.bias) * self.lr_multiplier\n",
    "    \n",
    "class AddNoise(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.initialized = False\n",
    "        self.register_parameter('bias',None)\n",
    "\n",
    "    def build(self, input : torch.Tensor):\n",
    "        n, c, h, w = input.shape[0]\n",
    "\n",
    "        self.bias = nn.Parameter(input.new(1,c,1,1).normal_())\n",
    "    \n",
    "    def forward(self, X):\n",
    "        if not self.initialized:\n",
    "            self.build(X)\n",
    "            self.initialized = True\n",
    "        \n",
    "        x, noise = X\n",
    "        return x + self.bias * noise\n",
    "\n",
    "class AdaIN(nn.Module):\n",
    "    def __init__(self, gain =1 , **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.gain = gain\n",
    "        self.initialized = False\n",
    "\n",
    "        self.register_module(\"dense_1\",None)\n",
    "        self.register_module(\"dense_2\",None)\n",
    "    \n",
    "    def build(self, input):\n",
    "        x, w = input\n",
    "\n",
    "        x_shape = x.shape\n",
    "        w_shape = w.shape\n",
    "\n",
    "        self.w_channels = w_shape[1]\n",
    "        self.x_channels = x_shape[1]\n",
    "\n",
    "        self.dense_1 = EqualizedDense(self.x_channels, gain =1 )\n",
    "        self.dense_2 = EqualizedDense(self.x_channels, gain =1 )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        if not self.initialized:\n",
    "            self.build(X)\n",
    "            self.initialized = True\n",
    "        x, w = X\n",
    "        ys = self.dense_1(w).reshape([-1,self.x_channels,1,1])\n",
    "        yb = self.dense_2(w).reshape([-1,self.x_channels,1,1])\n",
    "\n",
    "        return ys *x + yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mapping(nn.Module):\n",
    "    def __init__(self, num_stages, input_shape = 512) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_stages = num_stages\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "        layers =  []\n",
    "\n",
    "        for i in range(8):\n",
    "            layers.append(EqualizedDense(input_shape, 512,1, lr_multiplier=0.01))\n",
    "            layers.append(nn.LeakyReLU())\n",
    "        \n",
    "        self.layers  = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        x = self.layers(X)\n",
    "        return torch.tile(x.unsqueeze(1),(1,self.num_stages,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.9273, 0.9273, 0.9273, 0.9273, 0.9273],\n",
       "          [0.9273, 0.9273, 0.9273, 0.9273, 0.9273]],\n",
       "\n",
       "         [[1.8117, 1.8117, 1.8117, 1.8117, 1.8117],\n",
       "          [1.8117, 1.8117, 1.8117, 1.8117, 1.8117]]],\n",
       "\n",
       "\n",
       "        [[[0.9273, 0.9273, 0.9273, 0.9273, 0.9273],\n",
       "          [0.9273, 0.9273, 0.9273, 0.9273, 0.9273]],\n",
       "\n",
       "         [[1.8117, 1.8117, 1.8117, 1.8117, 1.8117],\n",
       "          [1.8117, 1.8117, 1.8117, 1.8117, 1.8117]]]], grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = AdaIN()\n",
    "\n",
    "A(torch.ones(20).view(2,1,2,5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
