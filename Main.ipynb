{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetRoot = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log2(x):\n",
    "    return int(math.log2(x))\n",
    "\n",
    "def plot_images(images, log2_res, fname=\"\"):\n",
    "    '''\n",
    "    Helper function to plot a set of images\n",
    "    '''\n",
    "    scales = {2: 0.5, 3: 1, 4: 2, 5: 3, 6: 4, 7: 5, 8: 6, 9: 7, 10: 8}\n",
    "    scale = scales[log2_res]\n",
    "\n",
    "    grid_col = min(images.shape[0], int(32 // scale))\n",
    "    grid_row = 1\n",
    "\n",
    "    f, axarr = plt.subplots(\n",
    "        grid_row, grid_col, figsize=(grid_col * scale, grid_row * scale)\n",
    "    )\n",
    "\n",
    "    for row in range(grid_row):\n",
    "        ax = axarr if grid_row == 1 else axarr[row]\n",
    "        for col in range(grid_col):\n",
    "            ax[col].imshow(images[row * grid_col + col])\n",
    "            ax[col].axis(\"off\")\n",
    "    plt.show()\n",
    "    if fname:\n",
    "        f.savefig(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:00<00:00, 199547.27it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_filepath(x):\n",
    "    x = x.split(\"/\")\n",
    "    x.pop(1)\n",
    "    return \"/\".join(x)\n",
    "\n",
    "data = pd.read_json(os.path.join(datasetRoot,\"ffhq-dataset-v2.json\"),orient=\"index\")\n",
    "data['image_path'] = data.progress_apply(lambda x : process_filepath(x['thumbnail']['file_path']),axis=1)\n",
    "data = data.drop(columns=['image','thumbnail','in_the_wild','metadata'])\n",
    "\n",
    "train_data = data.loc[data['category']=='training'].drop(columns=['category'])\n",
    "validation_data = data.loc[data['category']=='validation'].drop(columns=['category']).reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFHQDataset(Dataset):\n",
    "    \"\"\" Dataset to load FFHQ data from a dataframe\n",
    "\n",
    "    Args:\n",
    "\n",
    "        dataframe : dataframe with image paths in column 'image_path'\n",
    "        datasetRoot : the root path to join before the image_path, if any.\n",
    "        res : resolution of the image generated\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, datasetRoot, res = 2, device = \"cuda\") -> None:\n",
    "        super().__init__()\n",
    "        self.dataframe = dataframe\n",
    "        self.datasetRoot = datasetRoot\n",
    "        self.res = res\n",
    "        self.device = device\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "        img_path = os.path.join(self.datasetRoot,self.dataframe['image_path'].iloc[index])\n",
    "        image = read_image(img_path)\n",
    "        return Resize((self.res,self.res))(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 3, 2, 2])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = FFHQDataset(train_data,datasetRoot)\n",
    "validation_dataset = FFHQDataset(validation_data,datasetRoot)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "train_features = next(iter(train_dataloader)).cpu()\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "img = train_features[0][1]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fade_in(alpha, a, b):\n",
    "    '''\n",
    "    Smoothing function\n",
    "    '''\n",
    "    return alpha * a + (1- alpha ) * b\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    '''\n",
    "    Wasserstein Loss ( Refer to WGANs)\n",
    "    '''\n",
    "    return - (y_true * y_pred).mean()\n",
    "\n",
    "def pixel_norm(x, epsilon = 1e-8):\n",
    "    return x / torch.sqrt(torch.mean(x**2, dim = -1, keepdim=True)+ epsilon)\n",
    "\n",
    "def minibatch_std(x, group_size=4, num_new_features=1, eps=1e-8):\n",
    "    \"\"\"Compute the mini-batch standard deviation of input tensor x.\n",
    "        ref : <https://arxiv.org/pdf/1710.10196.pdf>\n",
    "    \"\"\"\n",
    "    batch_size, num_channels, height, width = x.shape\n",
    "    grouped_size = min(group_size, batch_size)\n",
    "    grouped_inputs = x.view(grouped_size, -1, num_channels, height, width)\n",
    "    \n",
    "    grouped_mean = grouped_inputs.mean(dim=0, keepdim=False)\n",
    "    squared_diffs = (grouped_inputs - grouped_mean)**2\n",
    "    grouped_var = squared_diffs.mean(dim=0, keepdim=False)\n",
    "    grouped_std = torch.sqrt(grouped_var + eps)\n",
    "    \n",
    "    avg_std = grouped_std.mean(dim=[1,2,3], keepdim=True)\n",
    "    tiled_avg_std = avg_std.repeat(grouped_size, 1, height, width)\n",
    "    new_features = torch.cat((x, tiled_avg_std), dim=1)\n",
    "    \n",
    "    return new_features\n",
    "\n",
    "class EqualisedConv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel = 3, gain = 2) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.w = nn.Parameter(torch.empty(out_channels, in_channels ,kernel, kernel).normal_(), requires_grad= True)\n",
    "        self.b = nn.Parameter(torch.empty(out_channels).zero_() , requires_grad= True)\n",
    "\n",
    "        fan_in = kernel * kernel * in_channels\n",
    "        self.scale = math.sqrt(gain / fan_in)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        return F.conv2d(inputs, self.scale * self.w, self.b, padding='same')\n",
    "\n",
    "\n",
    "class EqualisedLinear(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out , gain = 2, lr_multiplier = 1) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.w = nn.Parameter(torch.empty(dim_out,dim_in).normal_(0.0, 1.0 / lr_multiplier), requires_grad= True)\n",
    "        self.b = nn.Parameter(torch.empty(dim_out).zero_(),requires_grad=True)\n",
    "\n",
    "        self.scale = math.sqrt(gain / dim_in)\n",
    "        self.lr_multiplier = lr_multiplier\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        return F.linear(inputs, self.scale * self.w, self.b) * self.lr_multiplier\n",
    "\n",
    "class AddNoise(nn.Module):\n",
    "    def __init__(self, x_shape) -> None:\n",
    "        super().__init__()\n",
    "        c , h, w = x_shape\n",
    "        self.b = nn.Parameter(torch.empty([1,c,1,1]).normal_(),requires_grad=True)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x, noise = inputs\n",
    "        return  x + self.b * noise\n",
    "\n",
    "class AdaIn(nn.Module):\n",
    "    def __init__(self, w_channels, x_channels, gain =1) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.x_channels = x_channels\n",
    "\n",
    "        self.dense_1 = EqualisedLinear(w_channels, x_channels, gain)\n",
    "        self.dense_2 = EqualisedLinear(x_channels, x_channels, gain)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x, w = inputs\n",
    "        ys = self.dense_1(w).reshape((-1,self.x_channels,1,1))\n",
    "        yb = self.dense_2(w).reshape((-1,self.x_channels,1,1))\n",
    "\n",
    "        return ys * x + yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorBlock(nn.Module):\n",
    "    def __init__(self, filter_num, res, input_shape, is_base) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_base = is_base\n",
    "\n",
    "        # Needs AdaIn & AddNoise Layers\n",
    "        if not is_base:\n",
    "            self.up_sample = nn.Upsample(scale_factor=2)\n",
    "            self.init_conv = EqualisedConv2D(input_shape[0], filter_num, 3)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.add_noise_1 = AddNoise(input_shape)\n",
    "        self.instance_norm_1 = nn.InstanceNorm2d(input_shape[0])\n",
    "        self.ada_in_1 = AdaIn(512, input_shape[0])\n",
    "        self.conv_1 = EqualisedConv2D(input_shape[0], filter_num, 3)\n",
    "\n",
    "        self.add_noise_2 = AddNoise(input_shape)\n",
    "        self.instance_norm_2 = nn.InstanceNorm2d(input_shape[0])\n",
    "        self.ada_in_2 = AdaIn(512, input_shape[0])\n",
    "        self.conv_2 = EqualisedConv2D(input_shape[0], filter_num, 3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x, w, noise = inputs\n",
    "\n",
    "        if not self.is_base:\n",
    "            x = self.up_sample(x)\n",
    "            x = self.init_conv(x)\n",
    "\n",
    "        x = self.add_noise_1([x, noise])\n",
    "        x = self.instance_norm_1(x)\n",
    "        x = self.ada_in_1([x,w])\n",
    "        x = self.conv_1(x)\n",
    "\n",
    "        x = self.add_noise_2([x, noise])\n",
    "        x = self.instance_norm_2(x)\n",
    "        x = self.ada_in_2([x,w])\n",
    "        x = self.conv_2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "#### TEST Generate BLOCK ####\n",
    "# x = GeneratorBlock(512,4,[512,2,2],True)([torch.ones(24,512,2,2),torch.ones(24,512),torch.ones(1,2,2)])\n",
    "# x = EqualisedConv2D(512,3,1,gain = 1)(x)\n",
    "# x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 5, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DiscriminatorBlock(nn.Module):\n",
    "    def __init__(self, filter_num_1, filter_num_2, res, is_base = False) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_base = is_base\n",
    "        self.leakyRelu = nn.LeakyReLU(0.2)\n",
    "\n",
    "\n",
    "        if not is_base:\n",
    "            self.conv_1 = EqualisedConv2D(filter_num_1,filter_num_1,3)\n",
    "            self.conv_2 = EqualisedConv2D(filter_num_1,filter_num_2)\n",
    "            self.avg_pool = nn.AvgPool2d((2,2))\n",
    "        else:\n",
    "            self.conv_1 = EqualisedConv2D(filter_num_1+1,filter_num_1,3)\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.lin_1 = EqualisedLinear(res*res*filter_num_1,filter_num_1)\n",
    "            self.lin_2 = EqualisedLinear(filter_num_1,1)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # if not base, we don't flatten\n",
    "        if not self.is_base:\n",
    "            x = self.conv_1(inputs)\n",
    "            x = self.leakyRelu(x)\n",
    "            x = self.conv_2(x)\n",
    "            x = self.leakyRelu(x)\n",
    "            x = self.avg_pool(x)\n",
    "            return x\n",
    "        else:\n",
    "        # if base, we flatten & pass through dense\n",
    "            x = minibatch_std(inputs)\n",
    "            x = self.conv_1(x)\n",
    "            x = self.leakyRelu(x)\n",
    "            x = self.flatten(x)\n",
    "            x = self.lin_1(x)\n",
    "            x = self.leakyRelu(x)\n",
    "            x = self.lin_2(x)\n",
    "            return x\n",
    "        \n",
    "#### TEST Dicriminator BLOCK ####\n",
    "DiscriminatorBlock(5,5,4,True)(torch.ones(24,5,4,4)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator Module for the StyleGAN Generator\n",
    "    \"\"\"\n",
    "    def __init__(self, start_res_log2, target_res_log2) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.start_res_log2 = start_res_log2\n",
    "        self.target_res_log2 = target_res_log2\n",
    "        self.num_stages = target_res_log2 - start_res_log2 +1\n",
    "\n",
    "        self.curr_res_log2 = start_res_log2\n",
    "\n",
    "        # generator Blocks\n",
    "        self.g_blocks = []\n",
    "        # one rgb block per genblock\n",
    "        self.to_rgb = []\n",
    "\n",
    "        self.filter_nums = {\n",
    "            0: 512,\n",
    "            1: 512,\n",
    "            2: 512,  # 4x4\n",
    "            3: 512,  # 8x8\n",
    "            4: 512,  # 16x16\n",
    "            5: 512,  # 32x32\n",
    "            6: 256,  # 64x64\n",
    "            7: 128,  # 128x128\n",
    "            8: 64,  # 256x256\n",
    "            9: 32,  # 512x512\n",
    "            10: 16,\n",
    "        }\n",
    "\n",
    "        start_res = 2 ** start_res_log2\n",
    "        self.input_shape = (self.filter_nums[start_res_log2], start_res, start_res)\n",
    "        \n",
    "        for i in range(start_res_log2, target_res_log2 + 1):\n",
    "            filter_num = self.filter_nums[i]\n",
    "            res = 2 ** i\n",
    "            \n",
    "            to_rgb = EqualisedConv2D(filter_num , 3, 1, gain=1)\n",
    "\n",
    "            self.to_rgb.append(to_rgb)\n",
    "\n",
    "            is_base = i == self.start_res_log2\n",
    "\n",
    "            if is_base:\n",
    "                input_shape = (self.filter_nums[i - 1], res, res)\n",
    "            else:\n",
    "                input_shape = (self.filter_nums[i - 1], 2 ** (i - 1), 2 ** (i - 1))\n",
    "            \n",
    "            g_block = GeneratorBlock(filter_num, res, input_shape, is_base)\n",
    "            self.g_blocks.append(g_block)\n",
    "\n",
    "        self.up_sample = nn.Upsample(scale_factor=2)\n",
    "        self.g_blocks = nn.ModuleList(self.g_blocks)\n",
    "        self.to_rgb = nn.ModuleList(self.to_rgb)\n",
    "    \n",
    "    def grow(self, res_log2):\n",
    "        self.curr_res_log2 = res_log2\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        g_input, w, noise_inputs, alpha = inputs\n",
    "\n",
    "        \"\"\"\n",
    "        Shapes: \n",
    "            g_input -> self.filter_nums[start_res_log2], start_res, start_res\n",
    "            w -> self.num_stages, 512\n",
    "            noise_inputs -> self.num_stages, 1, res, res\n",
    "            alpha -> 1\n",
    "        \"\"\"\n",
    "        num_stages = self.curr_res_log2 - self.start_res_log2 +1\n",
    "\n",
    "        x = self.g_blocks[0]([g_input, w[:,0],noise_inputs[0]])\n",
    "\n",
    "        rgb = None\n",
    "        if num_stages==1:\n",
    "            rgb = self.to_rgb[0](x)\n",
    "        else:\n",
    "            for i in range(1,num_stages-1):\n",
    "                x = self.g_blocks[i]([x,w[:,i],noise_inputs[i]])\n",
    "            \n",
    "            old_rgb = self.to_rgb[num_stages-1](x)\n",
    "            old_rgb = self.up_sample(old_rgb)\n",
    "\n",
    "            i = num_stages -1\n",
    "            x = self.g_blocks[i]([x,w[:,i],noise_inputs[i]])\n",
    "\n",
    "            new_rgb = self.to_rgb[i](x)\n",
    "\n",
    "            rgb = fade_in(alpha[0],new_rgb, old_rgb)\n",
    "        \n",
    "        return rgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, start_res_log2, target_res_log2) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.start_res_log2 = start_res_log2\n",
    "        self.target_res_log2 = target_res_log2\n",
    "        self.curr_res_log2 = self.start_res_log2\n",
    "\n",
    "        self.num_stages = target_res_log2 - start_res_log2 +1\n",
    "\n",
    "        self.filter_nums = {\n",
    "            0: 512,\n",
    "            1: 512,\n",
    "            2: 512,  # 4x4\n",
    "            3: 512,  # 8x8\n",
    "            4: 512,  # 16x16\n",
    "            5: 512,  # 32x32\n",
    "            6: 256,  # 64x64\n",
    "            7: 128,  # 128x128\n",
    "            8: 64,  # 256x256\n",
    "            9: 32,  # 512x512train_\n",
    "            10: 16,\n",
    "        }\n",
    "\n",
    "        self.d_blocks = []\n",
    "\n",
    "        self.from_rgb = []\n",
    "\n",
    "        for res_log2 in range(self.start_res_log2, self.target_res_log2 +1):\n",
    "            res = 2**res_log2\n",
    "\n",
    "            filter_num = self.filter_nums[res_log2]\n",
    "\n",
    "            from_rgb = nn.Sequential(\n",
    "                EqualisedConv2D(3,filter_num,1),\n",
    "                nn.LeakyReLU(0.2)\n",
    "            )\n",
    "\n",
    "            self.from_rgb.append(from_rgb)\n",
    "\n",
    "            d_block = DiscriminatorBlock(filter_num, self.filter_nums[res_log2-1],res,len(self.d_blocks)==0)\n",
    "            self.d_blocks.append(d_block)\n",
    "            \n",
    "        self.avg_pool = nn.AvgPool2d((2,2))\n",
    "        self.from_rgb = nn.ModuleList(self.from_rgb)\n",
    "        self.d_blocks = nn.ModuleList(self.d_blocks)\n",
    "    \n",
    "    def grow(self, res_log2):\n",
    "        self.curr_res_log2 = res_log2\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        input_image, alpha = inputs\n",
    "\n",
    "        res = 2 ** self.curr_res_log2 \n",
    "        idx = self.curr_res_log2 - self.start_res_log2\n",
    "\n",
    "        x = self.from_rgb[idx](input_image)\n",
    "        x = self.d_blocks[idx](x)\n",
    "\n",
    "        if idx >0 :\n",
    "            idx -=1\n",
    "            downsized_image = self.avg_pool(input_image)\n",
    "            y = self.from_rgb[idx](downsized_image)\n",
    "            x = fade_in(alpha[0],x,y)\n",
    "\n",
    "            for i in range(idx, -1, -1):\n",
    "                x = self.d_blocks[i](x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mapping(nn.Module):\n",
    "    def __init__(self, num_stages, input_shape = 512) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_stages = num_stages\n",
    "\n",
    "        self.layers = []\n",
    "        for _ in range(8):\n",
    "            self.layers.append(EqualisedLinear(input_shape, 512, 0.01))\n",
    "            self.layers.append(nn.LeakyReLU(0.2))\n",
    "        \n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = pixel_norm(input)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return torch.tile(torch.unsqueeze(x,1),(1,self.num_stages,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phase(Enum):\n",
    "    TRANSITION = 1\n",
    "    STABLE = 2\n",
    "\n",
    "class OptimType(Enum):\n",
    "    ADAM = 1\n",
    "    \n",
    "class StyleGAN(nn.Module):\n",
    "    def __init__(self, z_dim = 512, target_res = 128, start_res = 4) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.z_dim = 512\n",
    "\n",
    "        self.target_res_log2 = log2(target_res)\n",
    "        self.start_res_log2 = log2(start_res)\n",
    "\n",
    "        self.curr_res_log2 = self.target_res_log2\n",
    "        self.num_stages = self.target_res_log2 - self.start_res_log2 +1\n",
    "\n",
    "        self.alpha = 1.0\n",
    "        self.train_step_counter = 0\n",
    "        \n",
    "\n",
    "        self.mapping = Mapping(self.num_stages, self.z_dim)\n",
    "        self.discriminator = Discriminator(self.start_res_log2, self.target_res_log2)\n",
    "        self.generator = Generator(self.start_res_log2,self.target_res_log2)\n",
    "\n",
    "        self.g_input_shape = self.generator.input_shape\n",
    "\n",
    "        self.phase = None\n",
    "\n",
    "        self.loss_weights = {\n",
    "            \"gradient_penalty\" : 10,\n",
    "            \"drift\" : 0.001\n",
    "        }\n",
    "    \n",
    "        \n",
    "    def grow_model(self, res):\n",
    "        res_log2 = log2(res)\n",
    "\n",
    "        self.generator.grow(res_log2)\n",
    "        self.discriminator.grow(res_log2)\n",
    "\n",
    "        self.curr_res_log2 = res_log2\n",
    "        print(f\"\\nModel resolution : {res}x{res}\")\n",
    "\n",
    "    def configure(self, steps_per_epoch, phase, res, d_optimizer : OptimType, g_optimizer: OptimType, *args, **kwargs):\n",
    "        self.loss_weights = kwargs.pop(\"loss_weights\",self.loss_weights)\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "        if res!= 2 ** self.curr_res_log2:\n",
    "            self.grow_model(res)\n",
    "            if d_optimizer == OptimType.ADAM:\n",
    "                self.d_optimizer = torch.optim.Adam(params = self.discriminator.parameters(), lr=1e-3, betas=(0.0,0.99),eps=1e-8)\n",
    "            if g_optimizer == OptimType.ADAM:\n",
    "                self.g_optimizer = torch.optim.Adam(params = self.generator.parameters(), lr=1e-3, betas=(0.0,0.99),eps=1e-8)\n",
    "        \n",
    "        self.train_step_counter = 0\n",
    "        self.phase = phase\n",
    "        self.d_loss_metric = torch.mean\n",
    "        self.g_loss_metric = torch.mean\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.d_loss_metric, self.g_loss_metric]\n",
    "    \n",
    "    def generate_noise(self,batch_size):\n",
    "        return [\n",
    "            torch.empty(batch_size,1, 2**res,2**res ).normal_()\n",
    "            for res in range(self.start_res_log2, self.target_res_log2+1)\n",
    "        ]\n",
    "    \n",
    "    def gradient_penalty(self, real_images : torch.Tensor, fake_images: torch.Tensor, alpha, batch_size):\n",
    "        epsilon = torch.empty(batch_size,1,1,1)\n",
    "\n",
    "        interpolated_images = epsilon * real_images + (1 - epsilon ) * fake_images\n",
    "        interpolated_images.requires_grad_(True)\n",
    "\n",
    "        mixed_scores = self.discriminator([interpolated_images,alpha])\n",
    "\n",
    "        loss_inter = wasserstein_loss(-torch.ones(batch_size), mixed_scores)\n",
    "\n",
    "        gradient = torch.autograd.grad(\n",
    "            inputs = interpolated_images,\n",
    "            outputs = loss_inter,\n",
    "            grad_outputs = torch.ones_like(loss_inter),\n",
    "            create_graph = True,\n",
    "            retain_graph = True \n",
    "        )[0]\n",
    "\n",
    "        gradient = gradient.view(gradient.shape[0],-1)\n",
    "        gradient_norm = gradient.norm(2, dim=1)\n",
    "        gradient_penalty = torch.mean((gradient_norm - 1)**2)\n",
    "        return gradient_penalty\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        self.g_optimizer.zero_grad()\n",
    "        self.train_step_counter +=1\n",
    "        \n",
    "        if self.phase == Phase.TRANSITION:\n",
    "            self.alpha = float(self.train_step_counter/ self.steps_per_epoch)\n",
    "        elif self.phase == Phase.STABLE:\n",
    "            self.alpha = 1.0\n",
    "        else:\n",
    "            raise NotImplementedError(\"Configure Model before training\")\n",
    "\n",
    "        alpha = torch.Tensor([self.alpha])\n",
    "        batch_size = real_images.shape[0]\n",
    "\n",
    "        real_labels = torch.ones(batch_size)\n",
    "        fake_labels = -torch.ones(batch_size)\n",
    "\n",
    "        z = torch.empty(batch_size, self.z_dim).normal_()\n",
    "        const_input = torch.ones(tuple([batch_size]+ list(self.g_input_shape)))\n",
    "        noise = self.generate_noise(batch_size)\n",
    "\n",
    "        w = self.mapping(z)\n",
    "        fake_images : torch.Tensor = self.generator([const_input, w, noise, alpha])\n",
    "\n",
    "        pred_fake = self.discriminator([fake_images, alpha])\n",
    "        \n",
    "        #### Generator Loss\n",
    "        g_loss = wasserstein_loss(real_labels, pred_fake)\n",
    "\n",
    "        self.generator.zero_grad()\n",
    "        g_loss.backward()\n",
    "        self.g_optimizer.step()\n",
    "\n",
    "        ### Discriminator Loss\n",
    "        pred_fake = self.discriminator([fake_images.detach(), alpha])\n",
    "        pred_real = self.discriminator([real_images.float(), alpha])\n",
    "\n",
    "        loss_fake = wasserstein_loss(fake_labels, pred_fake)\n",
    "        loss_real = wasserstein_loss(real_labels, pred_real)\n",
    "\n",
    "        gradient_penalty = self.gradient_penalty(real_images, fake_images.detach(), alpha, batch_size)\n",
    "\n",
    "        all_pred = torch.concat([pred_fake, pred_real], dim=0)\n",
    "        drift_loss = self.loss_weights[\"drift\"] * torch.mean(all_pred ** 2)\n",
    "        \n",
    "        d_loss = loss_fake + loss_real + gradient_penalty + drift_loss\n",
    "\n",
    "        self.discriminator.zero_grad()\n",
    "        d_loss.backward()\n",
    "        self.d_optimizer.step()\n",
    "\n",
    "        return {\n",
    "            \"g_loss\": torch.mean(g_loss),\n",
    "            \"d_loss\": torch.mean(d_loss)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_RES = 4\n",
    "TARGET_RES = 128\n",
    "\n",
    "style_gan = StyleGAN(start_res=START_RES, target_res= TARGET_RES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use different batch size for different resolution, so larger image size\n",
    "# could fit into GPU memory. The keys is image resolution in log2\n",
    "batch_sizes = {2: 16, 3: 16, 4: 16, 5: 16, 6: 16, 7: 8, 8: 4, 9: 2, 10: 1}\n",
    "# We adjust the train step accordingly\n",
    "train_step_ratio = {k: batch_sizes[2] / v for k, v in batch_sizes.items()}\n",
    "\n",
    "def train(start_res = START_RES, target_res = TARGET_RES, steps_per_epoch = 5000, display_images = True, datasetRoot = \"./\"):\n",
    "    val_batch_size = 16\n",
    "    val_z = torch.empty((val_batch_size, style_gan.z_dim)).normal_()\n",
    "    val_noise = style_gan.generate_noise(val_batch_size)\n",
    "\n",
    "    def process_filepath(x):\n",
    "        x = x.split(\"/\")\n",
    "        x.pop(1)\n",
    "        return \"/\".join(x)\n",
    "\n",
    "    data = pd.read_json(os.path.join(datasetRoot,\"ffhq-dataset-v2.json\"),orient=\"index\")\n",
    "    data['image_path'] = data.apply(lambda x : process_filepath(x['thumbnail']['file_path']),axis=1)\n",
    "    data = data.drop(columns=['image','thumbnail','in_the_wild','metadata'])\n",
    "\n",
    "    train_data = data.loc[data['category']=='training'].drop(columns=['category'])\n",
    "    validation_data = data.loc[data['category']=='validation'].drop(columns=['category']).reset_index().drop(columns=['index'])\n",
    "\n",
    "    \n",
    "    start_res_log2 = log2(start_res)\n",
    "    target_res_log2 = log2(target_res)\n",
    "\n",
    "    for res_log2 in range(start_res_log2, target_res_log2 + 1):\n",
    "        res = 2**res_log2\n",
    "\n",
    "        for phase in Phase:\n",
    "            if res==start_res and phase == Phase.TRANSITION:\n",
    "                continue\n",
    "            \n",
    "            train_dataloader = DataLoader(FFHQDataset(train_data, datasetRoot, res),batch_size= batch_sizes[res_log2], shuffle= True)\n",
    "            steps = int(train_step_ratio[res_log2] * steps_per_epoch)\n",
    "\n",
    "            style_gan.configure(\n",
    "                d_optimizer=OptimType.ADAM,\n",
    "                g_optimizer=OptimType.ADAM,\n",
    "                steps_per_epoch= steps,\n",
    "                res = res,\n",
    "                phase=phase\n",
    "            )\n",
    "\n",
    "            prefix = f\"res_{res}x{res}_{style_gan.phase}\"\n",
    "\n",
    "            print('Training '+prefix +\" model\")\n",
    "\n",
    "            g_loss = 0.0\n",
    "            d_loss = 0.0\n",
    "\n",
    "            ## Train for an epoch\n",
    "            for i, batch in tqdm(enumerate(train_dataloader),total=steps):\n",
    "                if i==steps:\n",
    "                    break\n",
    "\n",
    "                losses = style_gan.train_step(batch)\n",
    "\n",
    "                g_loss += losses['g_loss']\n",
    "                d_loss += losses['d_loss']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
